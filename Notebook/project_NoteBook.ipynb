{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainability of Image classifier\n",
    "\n",
    "The current notebook consists of the implementation of our project. It is organized as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import cv2\n",
    "import requests\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image,deprocess_image,preprocess_image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torchvision       import transforms, datasets, models\n",
    "import typing \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.backends.cudnn as cudnn\n",
    "from rise import *\n",
    "from PIL import ImageFile, Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "torch.manual_seed(12345)\n",
    "np.random.seed(12345)\n",
    "cudnn.benchmark = True\n",
    "%matplotlib inline\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dog csv file.\n",
    "dog_df = pd.read_csv(\"/Users/omer/Desktop/AMMI/Research_project/Implementation/Project_XAI/dogs_classes_ImageNet.csv\")\n",
    "class_indx=dog_df['ImageNet original class index '].to_list()\n",
    "imagenet_dataset = datasets.ImageFolder(\"/Users/omer/Desktop/AMMI/Research_project/Implementation/Image\")\n",
    "class_names=os.listdir(\"/Users/omer/Desktop/AMMI/Research_project/Implementation/Image\")\n",
    "class_names.remove('.DS_Store')\n",
    "class_names=[int(i) for i in class_names]\n",
    "class_names.sort()\n",
    "class_indxs= [class_indx[idx] for idx in class_names]\n",
    "class_indx=None\n",
    "class_nam = dog_df['class name'].to_list()\n",
    "class_nam1 = [ class_nam[int(x)] for x in class_names]\n",
    "#class_nam = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chihuahua', 'Japanese spaniel', 'Maltese dog, Maltese terrier, Maltese']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_nam1 #['Chihuahua', 'Japanese spaniel', 'Maltese dog, Maltese terrier, Maltese']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151, 152, 153]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_indxs #[151, 152, 153]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader):\n",
    "    \"\"\"Measures the accuracy of a model on a data set.\"\"\"\n",
    "\n",
    "    # Make sure the model is in evaluation mode.\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "\n",
    "    # We do not need to maintain intermediate activations while testing.\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Loop over test data.\n",
    "        for data, target in data_loader:\n",
    "\n",
    "            # Forward pass.\n",
    "            output = model(data.to(device))\n",
    "\n",
    "            # Get the label corresponding to the highest predicted probability.\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "            # Count number of correct predictions.\n",
    "            correct += pred.cpu().eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    # Print test accuracy.\n",
    "    percent = 100.0 * correct / len(data_loader.dataset)\n",
    "    print(f\"Accuracy: {correct}/{len(data_loader.dataset)} ({percent:.0f}%)\")\n",
    "    return percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def show(img):\n",
    "    \"\"\"Show PyTorch tensor img as an image in matplotlib.\"\"\"\n",
    "    npimg = img.cpu().detach().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation=\"nearest\")\n",
    "    plt.grid(False)\n",
    "    plt.gca().axis(\"off\")\n",
    "\n",
    "\n",
    "def display_thumb(img):\n",
    "    display.display(transforms.Resize(224)(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some of the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(12345)\n",
    "print(\"ImageNet classes:\", *imagenet_dataset.classes)\n",
    "\n",
    "# Show a random image and the corresponding target.\n",
    "for i in range(3):\n",
    "    img, target = random.choice(imagenet_dataset)\n",
    "    print(\n",
    "        \"Label of image: %d (%s). Original size: %s\"\n",
    "        % (target, class_nam[int(imagenet_dataset.classes[target])], img.size)\n",
    "    )\n",
    "\n",
    "    # Reduce image size by half to fit the images on the page :)\n",
    "    display.display(img.resize((img.size[0] // 2, img.size[1] // 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the default transform used in ImageNet models.\n",
    "inference_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# For visualization purposes we'll create a separate transform that operates in image space.\n",
    "inference_transform_show = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Reload out dataset with this transform.\n",
    "transformed_imagenet_dataset = datasets.ImageFolder(\n",
    "    \"/Users/omer/Desktop/AMMI/Research_project/Implementation/Image\",\n",
    "    transform=inference_transform,\n",
    ")\n",
    "\n",
    "transformed_imagenet_loader = torch.utils.data.DataLoader(\n",
    "    transformed_imagenet_dataset, batch_size=16, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained model.\n",
    "imagenet_alexnet = torchvision.models.alexnet(weights='IMAGENET1K_V1')\n",
    "\n",
    "# The loaded model is trained to classify image across 1000 classes. We have the classes in class_indx that we are going to consider.\n",
    "\n",
    "imagenet_alexnet.classifier[6].weight.data = imagenet_alexnet.classifier[6].weight.data[class_indxs]\n",
    "imagenet_alexnet.classifier[6].bias.data = imagenet_alexnet.classifier[6].bias.data[class_indxs]\n",
    "\n",
    "imagenet_alexnet = imagenet_alexnet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test AlexNet on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 547/559 (98%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "97.85330948121646"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model accuracy.\n",
    "test(imagenet_alexnet, transformed_imagenet_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistakes AlexNet made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_alexnet.eval()  # make sure the model is in evaluation mode\n",
    "mistak_alexnet = []\n",
    "for i in tqdm(range(len(transformed_imagenet_dataset))):\n",
    "    img, _ = imagenet_dataset[i]\n",
    "    tensor, target = transformed_imagenet_dataset[i]\n",
    "\n",
    "    \n",
    "    tensor = tensor.to(device)\n",
    "\n",
    "    _, prediction = imagenet_alexnet(tensor.unsqueeze(0)).squeeze(0).cpu().max(-1)\n",
    "    if prediction != target:\n",
    "        mistak_alexnet.append(i)\n",
    "        print(\n",
    "            \"Img id=%d. Excpected class %s, but predicted class %s.\"\n",
    "            % (\n",
    "                i,\n",
    "                class_nam[int(imagenet_dataset.classes[target])],\n",
    "                class_nam[int(imagenet_dataset.classes[prediction])],\n",
    "            )\n",
    "        )\n",
    "        display_thumb(img)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained model.\n",
    "imagenet_resnet = torchvision.models.resnet50(weights='IMAGENET1K_V1')\n",
    "\n",
    "# The loaded model is trained to classify image across 1000 classes. We have the classes in class_indx that we are going to consider.\n",
    "\n",
    "imagenet_resnet.fc.weight.data = imagenet_resnet.fc.weight.data[class_indxs]\n",
    "imagenet_resnet.fc.bias.data = imagenet_resnet.fc.bias.data[class_indxs]\n",
    "\n",
    "imagenet_resnet = imagenet_resnet.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test ResNet50 on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 558/559 (100%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99.82110912343471"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model accuracy.\n",
    "test(imagenet_resnet, transformed_imagenet_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistakes ResNet50 made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_resnet.eval()  # make sure the model is in evaluation mode\n",
    "mistak_resnet50 = []\n",
    "for i in tqdm(range(len(transformed_imagenet_dataset))):\n",
    "    img, _ = imagenet_dataset[i]\n",
    "    tensor, target = transformed_imagenet_dataset[i]\n",
    "\n",
    "    \n",
    "    tensor = tensor.to(device)\n",
    "\n",
    "    _, prediction = imagenet_resnet(tensor.unsqueeze(0)).squeeze(0).cpu().max(-1)\n",
    "    if prediction != target:\n",
    "        mistak_resnet50.append(i)\n",
    "        print(\n",
    "            \"Img id=%d. Excpected class %s, but predicted class %s.\"\n",
    "            % (\n",
    "                i,\n",
    "                class_nam[int(imagenet_dataset.classes[target])],\n",
    "                class_nam[int(imagenet_dataset.classes[prediction])],\n",
    "            )\n",
    "        )\n",
    "        display_thumb(img)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ResNet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained model.\n",
    "imagenet_resnet152 = torchvision.models.resnet152(weights='IMAGENET1K_V1')\n",
    "\n",
    "# The loaded model is trained to classify image across 1000 classes. We have the classes in class_indx that we are going to consider.\n",
    "\n",
    "imagenet_resnet152.fc.weight.data = imagenet_resnet152.fc.weight.data[class_indxs]\n",
    "imagenet_resnet152.fc.bias.data = imagenet_resnet152.fc.bias.data[class_indxs]\n",
    "\n",
    "imagenet_resnet152 = imagenet_resnet152.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test ResNet152 on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 557/559 (100%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99.6422182468694"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model accuracy.\n",
    "test(imagenet_resnet152, transformed_imagenet_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistakes ResNet152 made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_resnet152.eval()  # make sure the model is in evaluation mode\n",
    "mistak_resnet152 = []\n",
    "for i in tqdm(range(len(transformed_imagenet_dataset))):\n",
    "    img, _ = imagenet_dataset[i]\n",
    "    tensor, target = transformed_imagenet_dataset[i]\n",
    "\n",
    "    \n",
    "    tensor = tensor.to(device)\n",
    "\n",
    "    _, prediction = imagenet_resnet152(tensor.unsqueeze(0)).squeeze(0).cpu().max(-1)\n",
    "    if prediction != target:\n",
    "        mistak_resnet152.append(i)\n",
    "        print(\n",
    "            \"Img id=%d. Excpected class %s, but predicted class %s.\"\n",
    "            % (\n",
    "                i,\n",
    "                class_nam[int(imagenet_dataset.classes[target])],\n",
    "                class_nam[int(imagenet_dataset.classes[prediction])],\n",
    "            )\n",
    "        )\n",
    "        display_thumb(img)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ResNeXt101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/omer/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "# Load a pretrained model.\n",
    "imagenet_ResNeXt101 = torch.hub.load('pytorch/vision:v0.10.0', 'resnext101_32x8d', pretrained=True)#torchvision.models.resnet152(weights='IMAGENET1K_V1')\n",
    "\n",
    "# The loaded model is trained to classify image across 1000 classes. We have the classes in class_indx that we are going to consider.\n",
    "\n",
    "imagenet_ResNeXt101.fc.weight.data = imagenet_ResNeXt101.fc.weight.data[class_indxs]\n",
    "imagenet_ResNeXt101.fc.bias.data = imagenet_ResNeXt101.fc.bias.data[class_indxs]\n",
    "\n",
    "imagenet_ResNeXt101 = imagenet_ResNeXt101.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test ResNeXt101 on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 558/559 (100%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99.82110912343471"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model accuracy.\n",
    "test(imagenet_ResNeXt101, transformed_imagenet_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistakes ResNeXt101 made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_ResNeXt101.eval()  # make sure the model is in evaluation mode\n",
    "mistak_resNext101 = []\n",
    "for i in tqdm(range(len(transformed_imagenet_dataset))):\n",
    "    img, _ = imagenet_dataset[i]\n",
    "    tensor, target = transformed_imagenet_dataset[i]\n",
    "\n",
    "    \n",
    "    tensor = tensor.to(device)\n",
    "\n",
    "    _, prediction = imagenet_ResNeXt101(tensor.unsqueeze(0)).squeeze(0).cpu().max(-1)\n",
    "    if prediction != target:\n",
    "        mistak_resNext101.append(i)\n",
    "        print(\n",
    "            \"Img id=%d. Excpected class %s, but predicted class %s.\"\n",
    "            % (\n",
    "                i,\n",
    "                class_nam[int(imagenet_dataset.classes[target])],\n",
    "                class_nam[int(imagenet_dataset.classes[prediction])],\n",
    "            )\n",
    "        )\n",
    "        display_thumb(img)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load EfficientNet_b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/omer/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    }
   ],
   "source": [
    "# Load a pretrained model.\n",
    "imagenet_EfficientNet_b4 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b4', pretrained=True)\n",
    "\n",
    "# The loaded model is trained to classify image across 1000 classes. We have the classes in class_indx that we are going to consider.\n",
    "\n",
    "imagenet_EfficientNet_b4.classifier.fc.weight.data = imagenet_EfficientNet_b4.classifier.fc.weight.data[class_indxs]\n",
    "imagenet_EfficientNet_b4.classifier.fc.bias.data = imagenet_EfficientNet_b4.classifier.fc.bias.data[class_indxs]\n",
    "\n",
    "imagenet_EfficientNet_b4 = imagenet_EfficientNet_b4.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test EfficientNet_b4 on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 551/559 (99%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98.56887298747763"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model accuracy.\n",
    "test(imagenet_EfficientNet_b4, transformed_imagenet_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistakes EfficientNet_b4 made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_EfficientNet_b4.eval()  # make sure the model is in evaluation mode\n",
    "mistak_effic = []\n",
    "for i in tqdm(range(len(transformed_imagenet_dataset))):\n",
    "    img, _ = imagenet_dataset[i]\n",
    "    tensor, target = transformed_imagenet_dataset[i]\n",
    "\n",
    "    \n",
    "    tensor = tensor.to(device)\n",
    "\n",
    "    _, prediction = imagenet_EfficientNet_b4(tensor.unsqueeze(0)).squeeze(0).cpu().max(-1)\n",
    "    if prediction != target:\n",
    "        mistak_effic.append(i)\n",
    "        print(\n",
    "            \"Img id=%d. Excpected class %s, but predicted class %s.\"\n",
    "            % (\n",
    "                i,\n",
    "                class_nam[int(imagenet_dataset.classes[target])],\n",
    "                class_nam[int(imagenet_dataset.classes[prediction])],\n",
    "            )\n",
    "        )\n",
    "        display_thumb(img)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained model.\n",
    "imagenet_ViT =  torchvision.models.vit_b_16(weights='IMAGENET1K_V1')\n",
    "# The loaded model is trained to classify image across 1000 classes. We have the classes in class_indx that we are going to consider.\n",
    "imagenet_ViT.heads.head.weight.data = imagenet_ViT.heads.head.weight.data[class_indxs]\n",
    "imagenet_ViT.heads.head.bias.data = imagenet_ViT.heads.head.bias.data[class_indxs]\n",
    "\n",
    "imagenet_ViT = imagenet_ViT.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test ViT on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 558/559 (100%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99.82110912343471"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model accuracy.\n",
    "test(imagenet_ViT, transformed_imagenet_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistakes ViT made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_ViT.eval()  # make sure the model is in evaluation mode\n",
    "mistak_vit = []\n",
    "for i in tqdm(range(len(transformed_imagenet_dataset))):\n",
    "    img, _ = imagenet_dataset[i]\n",
    "    tensor, target = transformed_imagenet_dataset[i]\n",
    "\n",
    "    \n",
    "    tensor = tensor.to(device)\n",
    "\n",
    "    _, prediction = imagenet_ViT(tensor.unsqueeze(0)).squeeze(0).cpu().max(-1)\n",
    "    if prediction != target:\n",
    "        mistak_vit.append(i)\n",
    "        print(\n",
    "            \"Img id=%d. Excpected class %s, but predicted class %s.\"\n",
    "            % (\n",
    "                i,\n",
    "                class_nam[int(imagenet_dataset.classes[target])],\n",
    "                class_nam[int(imagenet_dataset.classes[prediction])],\n",
    "            )\n",
    "        )\n",
    "        display_thumb(img)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad-CAM:\n",
    "\n",
    "In this approach, we forward pass an image with its desirable label to the CNN until we got the probability of that label. Then, we set the gardients of all classes to zero except the desired class will be set to one. Then, we backpropagate to the rectified covolutional feature maps of interest. Finally, we combine these feature maps to get where the model has focused to give such prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_alexnet.eval()\n",
    "id=36\n",
    "img = np.array(imagenet_dataset[id][0])\n",
    "img = cv2.resize(img, (224, 224))\n",
    "img = np.float32(img) / 255\n",
    "input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# As usual for classication, the target is the logit output\n",
    "# before softmax, for that category.\n",
    "targets = [ClassifierOutputTarget( transformed_imagenet_dataset[id][1])]\n",
    "target_layers = [imagenet_alexnet.features]\n",
    "with GradCAM(model=imagenet_alexnet, target_layers=target_layers) as cam:\n",
    "    grayscale_cams = cam(input_tensor=input_tensor.to(device), targets=targets)\n",
    "    cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "images = np.hstack((np.uint8(255*img) , cam_image))\n",
    "Image.fromarray(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [54,142,136,117,198,153,191,220,248]:\n",
    "    img = np.array(imagenet_dataset[i][0])\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.float32(img) / 255\n",
    "    input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # As usual for classication, the target is the logit output\n",
    "    # before softmax, for that category.\n",
    "    targets = [ClassifierOutputTarget(transformed_imagenet_dataset[i][1])]\n",
    "    target_layers = [imagenet_alexnet.features]\n",
    "    with GradCAM(model=imagenet_alexnet, target_layers=target_layers) as cam:\n",
    "        grayscale_cams = cam(input_tensor=input_tensor.to(device), targets=targets)\n",
    "        cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "    images = np.hstack((np.uint8(255*img) , cam_image))\n",
    "    Image.fromarray(images).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mistakes AlexNet made \n",
    "for i in mistak_alexnet:\n",
    "    img = np.array(imagenet_dataset[i][0])\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.float32(img) / 255\n",
    "    input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # As usual for classication, the target is the logit output\n",
    "    # before softmax, for that category.\n",
    "    targets = [ClassifierOutputTarget(transformed_imagenet_dataset[i][1])]\n",
    "    target_layers = [imagenet_alexnet.features]\n",
    "    with GradCAM(model=imagenet_alexnet, target_layers=target_layers) as cam:\n",
    "        grayscale_cams = cam(input_tensor=input_tensor.to(device), targets=targets)\n",
    "        cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "    images = np.hstack((np.uint8(255*img) , cam_image))\n",
    "    Image.fromarray(images).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_resnet.eval()\n",
    "id=36\n",
    "img = np.array(imagenet_dataset[id][0])\n",
    "img = cv2.resize(img, (224, 224))\n",
    "img = np.float32(img) / 255\n",
    "input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# As usual for classication, the target is the logit output\n",
    "# before softmax, for that category.\n",
    "targets = [ClassifierOutputTarget( transformed_imagenet_dataset[id][1])]\n",
    "target_layers = [imagenet_resnet.layer4]\n",
    "with GradCAM(model=imagenet_resnet, target_layers=target_layers) as cam:\n",
    "    grayscale_cams = cam(input_tensor=input_tensor.to(device), targets=targets)\n",
    "    cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "images = np.hstack((np.uint8(255*img) , cam_image))\n",
    "Image.fromarray(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [54,142,136,117,198,153,191,220,248]:\n",
    "    img = np.array(imagenet_dataset[i][0])\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.float32(img) / 255\n",
    "    input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # As usual for classication, the target is the logit output\n",
    "    # before softmax, for that category.\n",
    "    targets = [ClassifierOutputTarget(transformed_imagenet_dataset[i][1])]\n",
    "    target_layers = [imagenet_resnet.layer4]\n",
    "    with GradCAM(model=imagenet_resnet, target_layers=target_layers) as cam:\n",
    "        grayscale_cams = cam(input_tensor=input_tensor.to(device), targets=targets)\n",
    "        cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "    images = np.hstack((np.uint8(255*img) , cam_image))\n",
    "    Image.fromarray(images).show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mistakes ResNet50 made \n",
    "for i in mistak_resnet50:\n",
    "    img = np.array(imagenet_dataset[i][0])\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.float32(img) / 255\n",
    "    input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # As usual for classication, the target is the logit output\n",
    "    # before softmax, for that category.\n",
    "    targets = [ClassifierOutputTarget(transformed_imagenet_dataset[i][1])]\n",
    "    target_layers = [imagenet_resnet.layer4]\n",
    "    with GradCAM(model=imagenet_resnet, target_layers=target_layers) as cam:\n",
    "        grayscale_cams = cam(input_tensor=input_tensor.to(device), targets=targets)\n",
    "        cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "    images = np.hstack((np.uint8(255*img) , cam_image))\n",
    "    Image.fromarray(images).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ResNet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_resnet152.eval()\n",
    "id=36\n",
    "img = np.array(imagenet_dataset[id][0])\n",
    "img = cv2.resize(img, (224, 224))\n",
    "img = np.float32(img) / 255\n",
    "input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# As usual for classication, the target is the logit output\n",
    "# before softmax, for that category.\n",
    "targets = [ClassifierOutputTarget( transformed_imagenet_dataset[id][1])]\n",
    "target_layers = [imagenet_resnet152.layer4]\n",
    "with GradCAM(model=imagenet_resnet152, target_layers=target_layers) as cam:\n",
    "    grayscale_cams = cam(input_tensor=input_tensor.to(device), targets=targets)\n",
    "    cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "images = np.hstack((np.uint8(255*img) , cam_image))\n",
    "Image.fromarray(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [54,142,136,117,198,153,191,220,248]:\n",
    "    img = np.array(imagenet_dataset[i][0])\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.float32(img) / 255\n",
    "    input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # As usual for classication, the target is the logit output\n",
    "    # before softmax, for that category.\n",
    "    targets = [ClassifierOutputTarget(transformed_imagenet_dataset[i][1])]\n",
    "    target_layers = [imagenet_resnet152.layer4]\n",
    "    with GradCAM(model=imagenet_resnet152, target_layers=target_layers) as cam:\n",
    "        grayscale_cams = cam(input_tensor=input_tensor.to(device), targets=targets)\n",
    "        cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "    images = np.hstack((np.uint8(255*img) , cam_image))\n",
    "    Image.fromarray(images).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mistakes ResNet152 made \n",
    "for i in mistak_resnet152:\n",
    "    img = np.array(imagenet_dataset[i][0])\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.float32(img) / 255\n",
    "    input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # As usual for classication, the target is the logit output\n",
    "    # before softmax, for that category.\n",
    "    targets = [ClassifierOutputTarget(transformed_imagenet_dataset[i][1])]\n",
    "    target_layers = [imagenet_resnet152.layer4]\n",
    "    with GradCAM(model=imagenet_resnet152, target_layers=target_layers) as cam:\n",
    "        grayscale_cams = cam(input_tensor=input_tensor.to(device), targets=targets)\n",
    "        cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "    images = np.hstack((np.uint8(255*img) , cam_image))\n",
    "    Image.fromarray(images).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ResNeXt101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_ResNeXt101.eval()\n",
    "id=36\n",
    "img = np.array(imagenet_dataset[id][0])\n",
    "img = cv2.resize(img, (224, 224))\n",
    "img = np.float32(img) / 255\n",
    "input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# As usual for classication, the target is the logit output\n",
    "# before softmax, for that category.\n",
    "targets = [ClassifierOutputTarget( transformed_imagenet_dataset[id][1])]\n",
    "target_layers = [imagenet_ResNeXt101.layer4]\n",
    "with GradCAM(model=imagenet_ResNeXt101, target_layers=target_layers) as cam:\n",
    "    grayscale_cams = cam(input_tensor=input_tensor.to(device), targets=targets)\n",
    "    cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "images = np.hstack((np.uint8(255*img) , cam_image))\n",
    "Image.fromarray(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [54,142,136,117,198,153,191,220,248]:\n",
    "    img = np.array(imagenet_dataset[i][0])\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.float32(img) / 255\n",
    "    input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # As usual for classication, the target is the logit output\n",
    "    # before softmax, for that category.\n",
    "    targets = [ClassifierOutputTarget(transformed_imagenet_dataset[i][1])]\n",
    "    target_layers = [imagenet_ResNeXt101.layer4]\n",
    "    with GradCAM(model=imagenet_ResNeXt101, target_layers=target_layers) as cam:\n",
    "        grayscale_cams = cam(input_tensor=input_tensor.to(device), targets=targets)\n",
    "        cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "    images = np.hstack((np.uint8(255*img) , cam_image))\n",
    "    Image.fromarray(images).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mistakes ResNeXt101 made \n",
    "for i in mistak_resNext101:\n",
    "    img = np.array(imagenet_dataset[i][0])\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.float32(img) / 255\n",
    "    input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # As usual for classication, the target is the logit output\n",
    "    # before softmax, for that category.\n",
    "    targets = [ClassifierOutputTarget(transformed_imagenet_dataset[i][1])]\n",
    "    target_layers = [imagenet_ResNeXt101.layer4]\n",
    "    with GradCAM(model=imagenet_ResNeXt101, target_layers=target_layers) as cam:\n",
    "        grayscale_cams = cam(input_tensor=input_tensor.to(device), targets=targets)\n",
    "        cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "    images = np.hstack((np.uint8(255*img) , cam_image))\n",
    "    Image.fromarray(images).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For EfficientNet_b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_EfficientNet_b4.eval()\n",
    "id=36\n",
    "img = np.array(imagenet_dataset[id][0])\n",
    "img = cv2.resize(img, (224, 224))\n",
    "img = np.float32(img) / 255\n",
    "input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# As usual for classication, the target is the logit output\n",
    "# before softmax, for that category.\n",
    "targets = [ClassifierOutputTarget( transformed_imagenet_dataset[id][1])]\n",
    "target_layers = [imagenet_EfficientNet_b4.layers[6]]\n",
    "with GradCAM(model=imagenet_EfficientNet_b4, target_layers=target_layers) as cam:\n",
    "    grayscale_cams = cam(input_tensor=input_tensor.to(device), targets=targets)\n",
    "    cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "images = np.hstack((np.uint8(255*img) , cam_image))\n",
    "Image.fromarray(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [54,142,136,117,198,153,191,220,248]:\n",
    "    img = np.array(imagenet_dataset[i][0])\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.float32(img) / 255\n",
    "    input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # As usual for classication, the target is the logit output\n",
    "    # before softmax, for that category.\n",
    "    targets = [ClassifierOutputTarget(transformed_imagenet_dataset[i][1])]\n",
    "    target_layers = [imagenet_EfficientNet_b4.layers[6]]\n",
    "    with GradCAM(model=imagenet_EfficientNet_b4, target_layers=target_layers) as cam:\n",
    "        grayscale_cams = cam(input_tensor=input_tensor.to(device), targets=targets)\n",
    "        cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "    images = np.hstack((np.uint8(255*img) , cam_image))\n",
    "    Image.fromarray(images).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mistakes EfficientNet made \n",
    "for i in mistak_effic:\n",
    "    img = np.array(imagenet_dataset[i][0])\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.float32(img) / 255\n",
    "    input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # As usual for classication, the target is the logit output\n",
    "    # before softmax, for that category.\n",
    "    targets = [ClassifierOutputTarget(transformed_imagenet_dataset[i][1])]\n",
    "    target_layers = [imagenet_EfficientNet_b4.layers[6]]\n",
    "    with GradCAM(model=imagenet_EfficientNet_b4, target_layers=target_layers) as cam:\n",
    "        grayscale_cams = cam(input_tensor=input_tensor.to(device), targets=targets)\n",
    "        cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "    images = np.hstack((np.uint8(255*img) , cam_image))\n",
    "    Image.fromarray(images).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Image Sampling for Explanations (RISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:08<00:00, 706.54it/s]\n"
     ]
    }
   ],
   "source": [
    "imagenet_alexnet = nn.Sequential(imagenet_alexnet, nn.Softmax(dim=1)).to(device).eval()\n",
    "for p in imagenet_alexnet.parameters():\n",
    "    p.requires_grad = False\n",
    "explainer = RISE(imagenet_alexnet)\n",
    "#Generate masks.\n",
    "explainer.generate_masks(Num_mask=6000, mask_size=8, prob=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(img: torch.Tensor,target:int) -> None:\n",
    "    #Make predictions with the created masks.\n",
    "    saliency = explainer(img.to(device)).cpu().numpy()\n",
    "    img = img[None]\n",
    "    #To get probability and prediction for the original image without masks.\n",
    "    prob , prediction = imagenet_alexnet(tensor[None].to(device)).squeeze(0).cpu().max(-1)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2,1)\n",
    "    plt.axis('off')\n",
    "    #The original image.\n",
    "    tensor_imshow(img[0])\n",
    "\n",
    "    plt.subplot(1, 2,2)\n",
    "    plt.axis('off')\n",
    "    tensor_imshow(img[0])\n",
    "    sal = saliency[prediction]\n",
    "    plt.imshow(sal, cmap='jet', alpha=0.5)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.suptitle(\"Img Prob.=%.2f. Excpected class: %s; Predicted class: %s.\"\n",
    "            % (\n",
    "                100*prob,\n",
    "                class_nam[int(imagenet_dataset.classes[target])],\n",
    "                class_nam[int(imagenet_dataset.classes[prediction])],\n",
    "            ))\n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=36\n",
    "tensor, target = transformed_imagenet_dataset[i]    \n",
    "show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [54,142,198,153,191,248]:\n",
    "    tensor, target = transformed_imagenet_dataset[i]    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [136,117,220]:\n",
    "    tensor, target = transformed_imagenet_dataset[i]    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mistakes AlexNet made\n",
    "for i in mistak_alexnet:\n",
    "    tensor, target = transformed_imagenet_dataset[i]    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(12345)\n",
    "for i in range(30):\n",
    "    tensor, target = random.choice(transformed_imagenet_dataset)    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:08<00:00, 695.34it/s]\n"
     ]
    }
   ],
   "source": [
    "imagenet_resnet = nn.Sequential(imagenet_resnet, nn.Softmax(dim=1)).to(device).eval()\n",
    "for p in imagenet_resnet.parameters():\n",
    "    p.requires_grad = False\n",
    "explainer = RISE(imagenet_resnet)\n",
    "#Generate masks.\n",
    "explainer.generate_masks(Num_mask=6000, mask_size=8, prob=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(img: torch.Tensor,target:int) -> None:\n",
    "    #Make predictions with the created masks.\n",
    "    saliency = explainer(img.to(device)).cpu().numpy()\n",
    "    img = img[None]\n",
    "    #To get probability and prediction for the original image without masks.\n",
    "    prob , prediction = imagenet_resnet(tensor[None].to(device)).squeeze(0).cpu().max(-1)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2,1)\n",
    "    plt.axis('off')\n",
    "    #The original image.\n",
    "    tensor_imshow(img[0])\n",
    "\n",
    "    plt.subplot(1, 2,2)\n",
    "    plt.axis('off')\n",
    "    tensor_imshow(img[0])\n",
    "    sal = saliency[prediction]\n",
    "    plt.imshow(sal, cmap='jet', alpha=0.5)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.suptitle(\"Img Prob.=%.2f. Excpected class: %s; Predicted class: %s.\"\n",
    "            % (\n",
    "                100*prob,\n",
    "                class_nam[int(imagenet_dataset.classes[target])],\n",
    "                class_nam[int(imagenet_dataset.classes[prediction])],\n",
    "            ))\n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=36\n",
    "tensor, target = transformed_imagenet_dataset[i]    \n",
    "show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [54,142,198,153,191,248]:\n",
    "    tensor, target = transformed_imagenet_dataset[i]    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [136,117,220]:\n",
    "    tensor, target = transformed_imagenet_dataset[i]    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mistakes ResNet50 made\n",
    "for i in mistak_resnet50:\n",
    "    tensor, target = transformed_imagenet_dataset[i]    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(12345)\n",
    "for i in range(30):\n",
    "    tensor, target = random.choice(transformed_imagenet_dataset)    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ResNet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:08<00:00, 685.18it/s]\n"
     ]
    }
   ],
   "source": [
    "imagenet_resnet152 = nn.Sequential(imagenet_resnet152, nn.Softmax(dim=1)).to(device).eval()\n",
    "for p in imagenet_resnet152.parameters():\n",
    "    p.requires_grad = False\n",
    "explainer = RISE(imagenet_resnet152)\n",
    "#Generate masks.\n",
    "explainer.generate_masks(Num_mask=6000, mask_size=8, prob=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(img: torch.Tensor,target:int) -> None:\n",
    "    #Make predictions with the created masks.\n",
    "    saliency = explainer(img.to(device)).cpu().numpy()\n",
    "    img = img[None]\n",
    "    #To get probability and prediction for the original image without masks.\n",
    "    prob , prediction = imagenet_resnet152(tensor[None].to(device)).squeeze(0).cpu().max(-1)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2,1)\n",
    "    plt.axis('off')\n",
    "    #The original image.\n",
    "    tensor_imshow(img[0])\n",
    "\n",
    "    plt.subplot(1, 2,2)\n",
    "    plt.axis('off')\n",
    "    tensor_imshow(img[0])\n",
    "    sal = saliency[prediction]\n",
    "    plt.imshow(sal, cmap='jet', alpha=0.5)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.suptitle(\"Img Prob.=%.2f. Excpected class: %s; Predicted class: %s.\"\n",
    "            % (\n",
    "                100*prob,\n",
    "                class_nam[int(imagenet_dataset.classes[target])],\n",
    "                class_nam[int(imagenet_dataset.classes[prediction])],\n",
    "            ))\n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=36\n",
    "tensor, target = transformed_imagenet_dataset[i]    \n",
    "show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [54,142,198,153,191,248]:\n",
    "    tensor, target = transformed_imagenet_dataset[i]    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [136,117,220]:\n",
    "    tensor, target = transformed_imagenet_dataset[i]    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mistakes ResNet152 made\n",
    "for i in mistak_resnet152:\n",
    "    tensor, target = transformed_imagenet_dataset[i]    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(12345)\n",
    "for i in range(30):\n",
    "    tensor, target = random.choice(transformed_imagenet_dataset)    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ResNeXt101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:08<00:00, 713.75it/s]\n"
     ]
    }
   ],
   "source": [
    "imagenet_ResNeXt101 = nn.Sequential(imagenet_ResNeXt101, nn.Softmax(dim=1)).to(device).eval()\n",
    "for p in imagenet_ResNeXt101.parameters():\n",
    "    p.requires_grad = False\n",
    "explainer = RISE(imagenet_ResNeXt101)\n",
    "#Generate masks.\n",
    "explainer.generate_masks(Num_mask=6000, mask_size=8, prob=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(img: torch.Tensor,target:int) -> None:\n",
    "    #Make predictions with the created masks.\n",
    "    saliency = explainer(img.to(device)).cpu().numpy()\n",
    "    img = img[None]\n",
    "    #To get probability and prediction for the original image without masks.\n",
    "    prob , prediction = imagenet_ResNeXt101(tensor[None].to(device)).squeeze(0).cpu().max(-1)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2,1)\n",
    "    plt.axis('off')\n",
    "    #The original image.\n",
    "    tensor_imshow(img[0])\n",
    "\n",
    "    plt.subplot(1, 2,2)\n",
    "    plt.axis('off')\n",
    "    tensor_imshow(img[0])\n",
    "    sal = saliency[prediction]\n",
    "    plt.imshow(sal, cmap='jet', alpha=0.5)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.suptitle(\"Img Prob.=%.2f. Excpected class: %s; Predicted class: %s.\"\n",
    "            % (\n",
    "                100*prob,\n",
    "                class_nam[int(imagenet_dataset.classes[target])],\n",
    "                class_nam[int(imagenet_dataset.classes[prediction])],\n",
    "            ))\n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=36\n",
    "tensor, target = transformed_imagenet_dataset[i]    \n",
    "show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [54,142,198,153,191,248]:\n",
    "    tensor, target = transformed_imagenet_dataset[i]    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [136,117,220]:\n",
    "    tensor, target = transformed_imagenet_dataset[i]    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mistakes ResNeXt101 made\n",
    "for i in mistak_resNext101:\n",
    "    tensor, target = transformed_imagenet_dataset[i]    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(12345)\n",
    "for i in range(30):\n",
    "    tensor, target = random.choice(transformed_imagenet_dataset)    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For EfficientNet_b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:08<00:00, 686.78it/s]\n"
     ]
    }
   ],
   "source": [
    "imagenet_EfficientNet_b4 = nn.Sequential(imagenet_EfficientNet_b4, nn.Softmax(dim=1)).to(device).eval()\n",
    "for p in imagenet_EfficientNet_b4.parameters():\n",
    "    p.requires_grad = False\n",
    "explainer = RISE(imagenet_EfficientNet_b4)\n",
    "#Generate masks.\n",
    "explainer.generate_masks(Num_mask=6000, mask_size=8, prob=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(img: torch.Tensor,target:int) -> None:\n",
    "    #Make predictions with the created masks.\n",
    "    saliency = explainer(img.to(device)).cpu().numpy()\n",
    "    img = img[None]\n",
    "    #To get probability and prediction for the original image without masks.\n",
    "    prob , prediction = imagenet_EfficientNet_b4(tensor[None].to(device)).squeeze(0).cpu().max(-1)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2,1)\n",
    "    plt.axis('off')\n",
    "    #The original image.\n",
    "    tensor_imshow(img[0])\n",
    "\n",
    "    plt.subplot(1, 2,2)\n",
    "    plt.axis('off')\n",
    "    tensor_imshow(img[0])\n",
    "    sal = saliency[prediction]\n",
    "    plt.imshow(sal, cmap='jet', alpha=0.5)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.suptitle(\"Img Prob.=%.2f. Excpected class: %s; Predicted class: %s.\"\n",
    "            % (\n",
    "                100*prob,\n",
    "                class_nam[int(imagenet_dataset.classes[target])],\n",
    "                class_nam[int(imagenet_dataset.classes[prediction])],\n",
    "            ))\n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=36\n",
    "tensor, target = transformed_imagenet_dataset[i]    \n",
    "show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [54,142,198,153,191,248]:\n",
    "    tensor, target = transformed_imagenet_dataset[i]    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [136,117,220]:\n",
    "    tensor, target = transformed_imagenet_dataset[i]    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mistakes EfficientNet_b4 made\n",
    "for i in mistak_effic:\n",
    "    tensor, target = transformed_imagenet_dataset[i]    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(12345)\n",
    "for i in range(30):\n",
    "    tensor, target = random.choice(transformed_imagenet_dataset)    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:08<00:00, 713.22it/s]\n"
     ]
    }
   ],
   "source": [
    "imagenet_ViT = nn.Sequential(imagenet_ViT, nn.Softmax(dim=1)).to(device).eval()\n",
    "for p in imagenet_ViT.parameters():\n",
    "    p.requires_grad = False\n",
    "explainer = RISE(imagenet_ViT)\n",
    "#Generate masks.\n",
    "explainer.generate_masks(Num_mask=6000, mask_size=8, prob=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(img: torch.Tensor,target:int) -> None:\n",
    "    #Make predictions with the created masks.\n",
    "    saliency = explainer(img.to(device)).cpu().numpy()\n",
    "    img = img[None]\n",
    "    #To get probability and prediction for the original image without masks.\n",
    "    prob , prediction = imagenet_ViT(tensor[None].to(device)).squeeze(0).cpu().max(-1)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2,1)\n",
    "    plt.axis('off')\n",
    "    #The original image.\n",
    "    tensor_imshow(img[0])\n",
    "\n",
    "    plt.subplot(1, 2,2)\n",
    "    plt.axis('off')\n",
    "    tensor_imshow(img[0])\n",
    "    sal = saliency[prediction]\n",
    "    plt.imshow(sal, cmap='jet', alpha=0.5)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.suptitle(\"Img Prob.=%.2f. Excpected class: %s; Predicted class: %s.\"\n",
    "            % (\n",
    "                100*prob,\n",
    "                class_nam[int(imagenet_dataset.classes[target])],\n",
    "                class_nam[int(imagenet_dataset.classes[prediction])],\n",
    "            ))\n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=36\n",
    "tensor, target = transformed_imagenet_dataset[i]    \n",
    "show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [54,142,198,153,191,248]:\n",
    "    tensor, target = transformed_imagenet_dataset[i]    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [136,117,220]:\n",
    "    tensor, target = transformed_imagenet_dataset[i]    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mistakes ViT made\n",
    "for i in mistak_vit:\n",
    "    tensor, target = transformed_imagenet_dataset[i]    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(12345)\n",
    "for i in range(30):\n",
    "    tensor, target = random.choice(transformed_imagenet_dataset)    \n",
    "    show_result(tensor,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "1- The dataset is downloaded from this site: https://www.dropbox.com/sh/yrfmp7hwa2w9gxz/AAATMrfWNLctPq1vnRa3mtZPa?dl=0 (accessed on August 14, 2023).\n",
    "\n",
    "About the data: https://github.com/megvii-research/FSSD_OoD_Detection/issues/1\n",
    "\n",
    "2- For RISE implementation, we relied mainly on this repository: https://github.com/eclique/RISE (accessed on August 14, 2023)\n",
    "\n",
    "3- For Grad-CAM, we relied mainly on this repository: https://github.com/jacobgil/pytorch-grad-cam (accessed on August 14, 2023)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
